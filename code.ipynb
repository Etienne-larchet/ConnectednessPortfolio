{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d8fe57",
   "metadata": {},
   "source": [
    "UTILS FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ce6cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adf3094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histo(df: pd.DataFrame | pd.Series, title: str, ylabel: str, xlabel: str = 'Date'):\n",
    "    if isinstance(df, pd.Series):\n",
    "        df = pd.DataFrame(df)\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(df, label=df.columns)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_heatmap(df: pd.DataFrame, title: str, ylabel: str, xlabel: str):\n",
    "    size = int(len(df) * 2/3)\n",
    "    plt.figure(figsize=(size, size-2))\n",
    "    sns.heatmap(\n",
    "        df,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cmap=\"viridis\",\n",
    "        xticklabels=df.columns,  # shock from\n",
    "        yticklabels=df.columns,  # affected variable\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9062eb08",
   "metadata": {},
   "source": [
    "DATA FETCHING & CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbecde66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a535191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download params\n",
    "START_DATE_OBS = \"2006-01-01\"\n",
    "END_DATE_OBS = \"2019-06-30\"\n",
    "END_DATE_BACKTESTING = \"2023-12-31\"\n",
    "TICKERS = [\n",
    "    \"ZT=F\",\n",
    "    \"ZS=F\",\n",
    "    \"ZR=F\",\n",
    "    \"ZO=F\",\n",
    "    \"ZN=F\",\n",
    "    \"ZM=F\",\n",
    "    \"ZL=F\",\n",
    "    \"ZF=F\",\n",
    "    # \"ZC=F\" Issue with covariance matrix (all values at 0)\n",
    "    \"ZB=F\",\n",
    "    \"YM=F\",\n",
    "    \"SIL=F\",\n",
    "    # \"SI=F\", # No data available in 2006\n",
    "    \"SB=F\",\n",
    "    # \"RTY=F\", # No data available in 2006\n",
    "    \"RB=F\",\n",
    "    \"PL=F\",\n",
    "    # \"PA=F\", # No data available in 2006\n",
    "    \"OJ=F\",\n",
    "    \"NQ=F\",\n",
    "    \"NG=F\",\n",
    "    # \"MGC=F\", # No data available in 2006\n",
    "    # \"LE=F\", # No data available in 2006\n",
    "    \"LBS=F\",\n",
    "    \"KE=F\",\n",
    "    \"KC=F\",\n",
    "    \"HO=F\",\n",
    "    \"HG=F\",\n",
    "    \"HE=F\",\n",
    "    \"GF=F\",\n",
    "    \"GC=F\",\n",
    "    \"ES=F\",\n",
    "    \"CT=F\",\n",
    "    \"CL=F\",\n",
    "    \"CC=F\",\n",
    "    # \"BZ=F\", # No data available in 2006\n",
    "    # \"B0=F\", # not found\n",
    "] # List of all commodities availables on YahooFinance the 04/06/2025\n",
    "\n",
    "# Download financial data\n",
    "df = yf.download(TICKERS, start=START_DATE_OBS, end=END_DATE_BACKTESTING, group_by=\"tickers\", auto_adjust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13ec539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "MISSING_THRESHOLD = 0.05\n",
    "\n",
    "inconsistent_tickers = [\n",
    "    ticker\n",
    "    for ticker in df.columns.levels[0]\n",
    "    if (\"Close\" in df[ticker].columns and df[(ticker, \"Close\")].isna().mean() > MISSING_THRESHOLD)\n",
    "]\n",
    "print(f\"Dropping tickers: {inconsistent_tickers} due to missing value above threshold of {MISSING_THRESHOLD:.0%}\")\n",
    "data = df.drop(columns=inconsistent_tickers, level=0)  # dropping tickers\n",
    "data = data.bfill()  # backfilling\n",
    "\n",
    "close = data.xs(\"Close\", axis=1, level=1)\n",
    "\n",
    "print(f\"Tickers: {list(close.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933a1c2f",
   "metadata": {},
   "source": [
    "DYIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15983631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "from statsmodels.tsa.vector_ar.var_model import VARResultsWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d4a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll(df: pd.DataFrame, window_size: int):\n",
    "    \"\"\"\n",
    "    Generates rolling windows of a DataFrame.\n",
    "    Yields:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A rolling window of the DataFrame.\n",
    "    \"\"\"\n",
    "    for i in range(len(df) - window_size):\n",
    "        yield df.iloc[i : i + window_size]\n",
    "\n",
    "def generalized_fevd(var_results: \"VARResultsWrapper\", horizon: int):\n",
    "    \"\"\"\n",
    "    Compute the Generalized Forecast Error Variance Decomposition (GFEVD)\n",
    "    for a fitted VAR model, as described by Diebold and Yilmaz. Implementation made by chatpgt and gemini\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    gfevd_matrix: np.ndarray\n",
    "        N x N matrix where element (i,j) is the contribution of shock j\n",
    "        to the forecast error variance of variable i.\n",
    "    \"\"\"\n",
    "    sigma_u = var_results.sigma_u  # Residual covariance matrix, shape (N, N)\n",
    "    if isinstance(sigma_u, pd.DataFrame):\n",
    "        sigma_u = sigma_u.values\n",
    "\n",
    "    var_coeffs = var_results.coefs  # VAR coefficient matrices, shape (lag, N, N)\n",
    "    p_lags, N, _ = var_coeffs.shape\n",
    "\n",
    "    # Compute MA coefficient matrices A_h for h=0,...,H-1\n",
    "    ma_coeffs = np.zeros((horizon, N, N))\n",
    "    ma_coeffs[0] = np.eye(N)\n",
    "    for h in range(1, horizon):\n",
    "        for k in range(1, p_lags + 1):\n",
    "            if h - k >= 0:\n",
    "                ma_coeffs[h] += np.dot(var_coeffs[k - 1], ma_coeffs[h - k])\n",
    "\n",
    "    # Calculate GFEVD (theta_ij elements before normalization)\n",
    "    gfevd_unnormalized = np.zeros((N, N))\n",
    "\n",
    "    # Calculate denominators\n",
    "    denominators = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        e_i = np.zeros(N)\n",
    "        e_i[i] = 1.0\n",
    "        for h in range(horizon):  # Sum from h=0 to H-1\n",
    "            term = np.dot(ma_coeffs[h], np.dot(sigma_u, ma_coeffs[h].T))\n",
    "            denominators[i] += np.dot(e_i.T, np.dot(term, e_i))\n",
    "\n",
    "    # Calculate each element (i,j) of the GFEVD matrix\n",
    "    for i in range(N):\n",
    "        e_i = np.zeros(N)\n",
    "        e_i[i] = 1.0\n",
    "        for j in range(N):\n",
    "            e_j = np.zeros(N)\n",
    "            e_j[j] = 1.0\n",
    "\n",
    "            sigma_jj_inv = 1.0 / sigma_u[j, j]\n",
    "            if np.isinf(sigma_jj_inv) or np.isnan(sigma_jj_inv):  # Handle potential division by zero if sigma_jj is 0\n",
    "                print(\n",
    "                    f\"Warning: sigma_jj for variable {j} is zero or very small. Setting sigma_jj_inv to a large number.\"\n",
    "                )\n",
    "                sigma_jj_inv = 1e12  # A large number to avoid NaN, but indicates an issue\n",
    "\n",
    "            numerator_sum_sq = 0.0\n",
    "            for h in range(horizon):\n",
    "                val = np.dot(e_i.T, np.dot(ma_coeffs[h], np.dot(sigma_u, e_j)))\n",
    "                numerator_sum_sq += val**2\n",
    "\n",
    "            if denominators[i] == 0:\n",
    "                # This implies the forecast error variance of variable i is zero, problematic.\n",
    "                print(f\"Warning: Denominator for GFEVD for variable {i} is zero. Setting GFEVD contribution to 0.\")\n",
    "                gfevd_unnormalized[i, j] = 0.0\n",
    "            else:\n",
    "                gfevd_unnormalized[i, j] = (sigma_jj_inv * numerator_sum_sq) / denominators[i]\n",
    "\n",
    "    # Normalize rows to sum to 1 to get the Pairwise Connectedness Matrix C^H_{i <- j}\n",
    "    row_sums = gfevd_unnormalized.sum(axis=1, keepdims=True)\n",
    "    # Handle cases where a row sum is zero (e.g., if a variable has zero forecast error variance)\n",
    "    safe_row_sums = np.where(row_sums == 0, 1, row_sums)  # Replace 0 with 1 to avoid NaN, result will be 0.\n",
    "    gfevd_normalized = gfevd_unnormalized / safe_row_sums\n",
    "    gfevd_normalized = np.where(row_sums == 0, 0, gfevd_normalized)  # Ensure original zero rows are zero\n",
    "\n",
    "    return gfevd_normalized\n",
    "\n",
    "def get_dyci(\n",
    "    returns: pd.DataFrame, rolling_window: int, forecast_horizon: int, max_lag: int\n",
    ") -> tuple[pd.Series, np.ndarray]:\n",
    "    tci = []  # Total Connectedness Index values\n",
    "    gfevd_matrices = []  # GFEVD matrices for each window\n",
    "\n",
    "    # Iterate through rolling windows\n",
    "    counter = 0\n",
    "    for window_data in roll(returns, rolling_window):\n",
    "        model = VAR(window_data)\n",
    "        results = model.fit(maxlags=max_lag)\n",
    "\n",
    "        # Calculate Generalized Forecast Error Variance Decomposition (GFEVD)\n",
    "        gfevd_matrix = generalized_fevd(results, forecast_horizon)\n",
    "        gfevd_matrices.append(gfevd_matrix)\n",
    "\n",
    "        # Calculate Total Connectedness Index (TCI)\n",
    "        total_connectedness = 100 * (1 - np.trace(gfevd_matrix) / len(returns.columns))\n",
    "        tci.append(total_connectedness)\n",
    "        if counter % 200 == 0:\n",
    "            print(f\"Lag {counter} completed in DYCI ({counter / len(returns):.2%})\")\n",
    "        counter += 1\n",
    "\n",
    "    # Create a pandas Series for the TCI for easier plotting\n",
    "    tci_series = pd.Series(tci, index=returns.index[rolling_window : rolling_window + len(tci)], name='TCI')\n",
    "    return tci_series, np.array(gfevd_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94a4231",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters for DYIC as per https://financialconnectedness.org/info_stock.html\n",
    "ROLLING_WINDOW = 150\n",
    "FORECAST_HORIZON = 10\n",
    "MAX_LAG = 3\n",
    "\n",
    "# Remove model VAR Warning -> ValueWarning: No frequency information was provided, so inferred frequency B will be used.\n",
    "warnings.filterwarnings(\"ignore\", category=ValueWarning)\n",
    "\n",
    "returns = np.log(close / close.shift()).dropna()\n",
    "\n",
    "close_obs = close.loc[:END_DATE_OBS]\n",
    "returns_obs = returns.loc[:END_DATE_OBS]\n",
    "\n",
    "tci_series, gfevd_matrices = get_dyci(returns_obs, ROLLING_WINDOW, FORECAST_HORIZON, MAX_LAG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1719116",
   "metadata": {},
   "outputs": [],
   "source": [
    "gfevd_avg = pd.DataFrame(np.mean(gfevd_matrices, axis=0), index=close_obs.columns, columns=close_obs.columns)\n",
    "\n",
    "plot_histo(tci_series, title='Dynamic Total Connectedness Index of Commodities Markets', ylabel='Connectedness (%)')\n",
    "plot_heatmap(gfevd_avg, title=f\"Average GFEVD Impact Matrix (h={FORECAST_HORIZON})\", ylabel=\"Affected Variable\", xlabel=\"Shock From\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85840a12",
   "metadata": {},
   "source": [
    "PORTFOLIO OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011f3b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_BONDS =(0, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31c99e3",
   "metadata": {},
   "source": [
    "PORTFOLIO OPTIMIZATION USING AVERAGE DYIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ea9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypfopt import EfficientFrontier\n",
    "from pypfopt.expected_returns import mean_historical_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4618fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = mean_historical_return(close_obs)\n",
    "\n",
    "ef = EfficientFrontier(mu, gfevd_avg, weight_bounds=WEIGHT_BONDS)\n",
    "w = ef.max_sharpe()\n",
    "\n",
    "w_ptf_dyic_avg = pd.Series({ticker: w for ticker, w in w.items() if w > 0}, name='dyic_opt_w_avg')\n",
    "w_ptf_dyic_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d022f045",
   "metadata": {},
   "source": [
    "PORTFOLIO OPTIMIZATION USING COVARIANCE MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82582bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypfopt.risk_models import risk_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c244e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_matrix = risk_matrix(close_obs)\n",
    "plot_heatmap(cov_matrix, title='Covariance matrix', ylabel='ticker', xlabel='ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa701ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ef = EfficientFrontier(mu, cov_matrix, weight_bounds=WEIGHT_BONDS)\n",
    "w = ef.max_sharpe()\n",
    "\n",
    "w_ptf_cov = pd.Series({ticker: w for ticker, w in w.items() if w > 0.001}, name='covariance_marix_opt_w')  # Removing e-16 values \n",
    "w_ptf_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c83b112",
   "metadata": {},
   "source": [
    "PORTFOLIO OPTIMIZATION USING crisis-focus DYIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc78b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_crisis = returns.loc[\"2008-06-01\":\"2009-10-31\"]\n",
    "_, gfevd_matrices_crisis = get_dyci(returns_crisis, ROLLING_WINDOW, FORECAST_HORIZON, MAX_LAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a03ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "gfevd_crisis = pd.DataFrame(np.mean(gfevd_matrices_crisis, axis=0), index=returns_crisis.columns, columns=returns_crisis.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33686128",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(gfevd_crisis, title=f\"GFEVD Impact Matrix Crisis-focused (h={FORECAST_HORIZON})\", ylabel=\"Affected Variable\", xlabel=\"Shock From\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbae6f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = mean_historical_return(close_obs)\n",
    "\n",
    "ef = EfficientFrontier(mu, gfevd_crisis, weight_bounds=WEIGHT_BONDS)\n",
    "w = ef.max_sharpe()\n",
    "\n",
    "w_ptf_dyic_crisis = pd.Series({ticker: w for ticker, w in w.items() if w > 0}, name='dyic_opt_w_crisis')\n",
    "w_ptf_dyic_crisis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cdc5b0",
   "metadata": {},
   "source": [
    "MODEL COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55a5ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptf_cov_returns = returns.loc[END_DATE_OBS:, w_ptf_cov.keys()] @ w_ptf_cov\n",
    "ptf_dyic_avg_returns = returns.loc[END_DATE_OBS:, w_ptf_dyic_avg.keys()] @ w_ptf_dyic_avg\n",
    "ptf_equi_returns = returns.loc[END_DATE_OBS:] @ np.ones(len(returns.columns)) / len(returns.columns)\n",
    "ptf_dyic_crisis_returns = returns.loc[END_DATE_OBS:, w_ptf_dyic_crisis.keys()] @ w_ptf_dyic_crisis\n",
    "\n",
    "ptf_cov_returns.name = 'covariance'\n",
    "ptf_dyic_avg_returns.name =  'dyic_avg'\n",
    "ptf_dyic_crisis_returns.name =  'dyic_crisis'\n",
    "ptf_equi_returns.name =  'equi_weighted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb140215",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptf_cov_val = np.exp(np.cumsum(ptf_cov_returns))\n",
    "ptf_dyic_avg_val = np.exp(np.cumsum(ptf_dyic_avg_returns))\n",
    "ptf_dyic_crisis_val = np.exp(np.cumsum(ptf_dyic_crisis_returns))\n",
    "ptf_equi_val = np.exp(np.cumsum(ptf_equi_returns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56923708",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptf_values = pd.concat([ptf_cov_val, ptf_dyic_avg_val, ptf_dyic_crisis_val, ptf_equi_val], axis=1)\n",
    "\n",
    "plot_histo(ptf_values, 'Comparison portfolio optimization', 'ptf value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf20134",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptf_values.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592947ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptf_values[\"2020-01-01\":\"2020-06-30\"].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
